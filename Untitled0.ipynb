{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jokefun022/Google-Colab/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "V-iic7tdFHjT",
        "outputId": "83cf310f-a73a-4f08-c4e2-0ca948d375f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "tensorflow-text 2.19.0 requires tensorflow<2.20,>=2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "tf-keras 2.19.0 requires tensorflow<2.20,>=2.19, but you have tensorflow 2.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mTF: 2.20.0\n",
            "Torch: 2.8.0+cu126\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-71161548-ee0e-4ecc-8284-daeadeb6c3f1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-71161548-ee0e-4ecc-8284-daeadeb6c3f1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-607457331.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Option B: interactive upload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mcsv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# first uploaded file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    173\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "# ==============================#\n",
        "#  Colab Setup & Dependencies   #\n",
        "# ==============================#\n",
        "!pip -q install -U scikit-learn imbalanced-learn emoji transformers datasets accelerate xgboost\n",
        "!pip -q install -U tensorflow  # Colab usually has TF preinstalled; this ensures recent 2.x\n",
        "\n",
        "import os, re, math, json, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    Trainer, TrainingArguments, DataCollatorWithPadding\n",
        ")\n",
        "from datasets import Dataset\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "print(\"TF:\", tf.__version__)\n",
        "print(\"Torch:\", torch.__version__)\n",
        "\n",
        "# ==============================#\n",
        "#        Data Loading           #\n",
        "# ==============================#\n",
        "\n",
        "# Option A: direct path if already in the Colab VM (e.g., uploaded in left Files panel)\n",
        "PREFERRED_PATHS = [\n",
        "    '/content/Complete Data With Emoji.csv',         # typical Colab upload path\n",
        "    '/content/drive/MyDrive/Complete Data With Emoji.csv',  # Google Drive common location\n",
        "    '/content/Complete_Data_With_Emoji.csv',         # fallback naming\n",
        "    '/content/dataset.csv'\n",
        "]\n",
        "\n",
        "csv_path = None\n",
        "for p in PREFERRED_PATHS:\n",
        "    if os.path.exists(p):\n",
        "        csv_path = p\n",
        "        break\n",
        "\n",
        "if csv_path is None:\n",
        "    # Option B: interactive upload\n",
        "    from google.colab import files\n",
        "    up = files.upload()\n",
        "    csv_path = list(up.keys())[0]  # first uploaded file\n",
        "\n",
        "print(\"Using CSV:\", csv_path)\n",
        "\n",
        "# Read with robust encoding fallbacks\n",
        "def read_csv_robust(path):\n",
        "    for enc in ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252']:\n",
        "        try:\n",
        "            return pd.read_csv(path, encoding=enc)\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "    raise last_err\n",
        "\n",
        "df = read_csv_robust(csv_path)\n",
        "print(\"Shape:\", df.shape)\n",
        "df.head(3)\n",
        "\n",
        "# ==============================#\n",
        "#   Column Auto-Detection       #\n",
        "# ==============================#\n",
        "\n",
        "TEXT_CANDIDATES  = ['text','tweet','comment','sentence','content','message','body','post','clean_text','raw_text']\n",
        "LABEL_CANDIDATES = ['label','target','class','sentiment','category','y','labels','tag']\n",
        "\n",
        "def autodetect_columns(frame):\n",
        "    cols_lower = {c.lower(): c for c in frame.columns}\n",
        "    text_col = next((cols_lower[c] for c in TEXT_CANDIDATES if c in cols_lower), None)\n",
        "    label_col = next((cols_lower[c] for c in LABEL_CANDIDATES if c in cols_lower), None)\n",
        "\n",
        "    # If label missing but a small integer-like column exists, guess it\n",
        "    if label_col is None:\n",
        "        for c in frame.columns:\n",
        "            if frame[c].dtype in [np.int64, np.int32] or pd.api.types.is_integer_dtype(frame[c]):\n",
        "                if frame[c].nunique() <= max(20, int(len(frame)*0.05)+2):  # likely a label\n",
        "                    label_col = c\n",
        "                    break\n",
        "\n",
        "    # If text missing, pick first object column with long-ish strings\n",
        "    if text_col is None:\n",
        "        obj_cols = [c for c in frame.columns if frame[c].dtype == object]\n",
        "        if obj_cols:\n",
        "            text_col = max(obj_cols, key=lambda c: frame[c].astype(str).str.len().mean())\n",
        "\n",
        "    return text_col, label_col\n",
        "\n",
        "TEXT_COL, LABEL_COL = autodetect_columns(df)\n",
        "print(\"Detected TEXT_COL:\", TEXT_COL, \"| LABEL_COL:\", LABEL_COL)\n",
        "\n",
        "assert TEXT_COL is not None, \"Couldn't detect the text column. Please rename your text column to something like 'text' and rerun.\"\n",
        "assert LABEL_COL is not None, \"Couldn't detect the label column. Please rename your label column to something like 'label' and rerun.\"\n",
        "\n",
        "# Drop rows with missing\n",
        "df = df.dropna(subset=[TEXT_COL, LABEL_COL]).reset_index(drop=True)\n",
        "\n",
        "# ==============================#\n",
        "#     Basic Preprocessing       #\n",
        "# ==============================#\n",
        "\n",
        "URL_RE   = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "USER_RE  = re.compile(r'@\\w+')\n",
        "HASH_RE  = re.compile(r'#(\\w+)')\n",
        "WS_RE    = re.compile(r'\\s+')\n",
        "\n",
        "def clean_text_keep_emojis(s: str) -> str:\n",
        "    s = str(s)\n",
        "    s = URL_RE.sub(' ', s)\n",
        "    s = USER_RE.sub(' ', s)\n",
        "    # keep hashtag word, drop '#'\n",
        "    s = HASH_RE.sub(r'\\1', s)\n",
        "    s = s.lower()\n",
        "    s = WS_RE.sub(' ', s).strip()\n",
        "    return s\n",
        "\n",
        "df['clean'] = df[TEXT_COL].apply(clean_text_keep_emojis)\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "df['y'] = le.fit_transform(df[LABEL_COL])\n",
        "id2label = {i: lab for i, lab in enumerate(le.classes_)}\n",
        "label2id = {lab: i for i, lab in enumerate(le.classes_)}\n",
        "num_labels = len(le.classes_)\n",
        "print(\"Classes:\", le.classes_)\n",
        "\n",
        "# Train/Val split\n",
        "train_df, test_df = train_test_split(\n",
        "    df[['clean','y']], test_size=0.2, random_state=RANDOM_SEED, stratify=df['y']\n",
        ")\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df  = test_df.reset_index(drop=True)\n",
        "\n",
        "# ==============================#\n",
        "#    Helper: Evaluation         #\n",
        "# ==============================#\n",
        "\n",
        "def evaluate_and_report(y_true, y_pred, model_name, labels_map):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1m = f1_score(y_true, y_pred, average='macro')\n",
        "    print(f\"\\n[{model_name}]  Accuracy: {acc:.4f}  |  Macro-F1: {f1m:.4f}\")\n",
        "    print(classification_report(y_true, y_pred, target_names=[labels_map[i] for i in sorted(labels_map)]))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    return {\"model\": model_name, \"accuracy\": acc, \"macro_f1\": f1m}\n",
        "\n",
        "metrics_log = []\n",
        "\n",
        "# ==============================#\n",
        "#   Classic ML (TF-IDF)        #\n",
        "# ==============================#\n",
        "\n",
        "X_train = train_df['clean'].values\n",
        "y_train = train_df['y'].values\n",
        "X_test  = test_df['clean'].values\n",
        "y_test  = test_df['y'].values\n",
        "\n",
        "# TF-IDF: mix word + char n-grams (works well for Roman Urdu + emojis)\n",
        "tfidf = TfidfVectorizer(\n",
        "    analyzer='char_wb', ngram_range=(3,5), min_df=2, max_features=200000\n",
        ")\n",
        "tfidf_words = TfidfVectorizer(\n",
        "    analyzer='word', ngram_range=(1,2), min_df=2, max_features=200000\n",
        ")\n",
        "\n",
        "# We concatenate char and word features by stacking in a FeatureUnion-like way\n",
        "from scipy.sparse import hstack\n",
        "Xtr_char = tfidf.fit_transform(X_train)\n",
        "Xte_char = tfidf.transform(X_test)\n",
        "Xtr_word = tfidf_words.fit_transform(X_train)\n",
        "Xte_word = tfidf_words.transform(X_test)\n",
        "Xtr = hstack([Xtr_char, Xtr_word]).tocsr()\n",
        "Xte = hstack([Xte_char, Xte_word]).tocsr()\n",
        "\n",
        "# Optional: Address imbalance with RandomOverSampler on features\n",
        "ros = RandomOverSampler(random_state=RANDOM_SEED)\n",
        "Xtr_bal, ytr_bal = ros.fit_resample(Xtr, y_train)\n",
        "\n",
        "# 1) Multinomial Naive Bayes\n",
        "nb = MultinomialNB(alpha=0.5)\n",
        "nb.fit(Xtr_bal, ytr_bal)\n",
        "pred_nb = nb.predict(Xte)\n",
        "metrics_log.append(evaluate_and_report(y_test, pred_nb, \"MultinomialNB (TF-IDF)\", id2label))\n",
        "\n",
        "# 2) Linear SVM (LinearSVC)\n",
        "svm = LinearSVC()\n",
        "svm.fit(Xtr_bal, ytr_bal)\n",
        "pred_svm = svm.predict(Xte)\n",
        "metrics_log.append(evaluate_and_report(y_test, pred_svm, \"LinearSVC (TF-IDF)\", id2label))\n",
        "\n",
        "# 3) Logistic Regression (LBFGS, multinomial)\n",
        "lr = LogisticRegression(max_iter=2000, n_jobs=-1)\n",
        "lr.fit(Xtr_bal, ytr_bal)\n",
        "pred_lr = lr.predict(Xte)\n",
        "metrics_log.append(evaluate_and_report(y_test, pred_lr, \"LogisticRegression (TF-IDF)\", id2label))\n",
        "\n",
        "# 4) Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=RANDOM_SEED)\n",
        "rf.fit(Xtr_bal, ytr_bal)\n",
        "pred_rf = rf.predict(Xte)\n",
        "metrics_log.append(evaluate_and_report(y_test, pred_rf, \"RandomForest (TF-IDF)\", id2label))\n",
        "\n",
        "# 5) (Optional) XGBoost on TF-IDF\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    xgb = XGBClassifier(\n",
        "        n_estimators=600, max_depth=8, learning_rate=0.1, subsample=0.9, colsample_bytree=0.8,\n",
        "        reg_lambda=1.0, objective='multi:softprob', num_class=num_labels, n_jobs=-1,\n",
        "        random_state=RANDOM_SEED, tree_method='hist'\n",
        "    )\n",
        "    xgb.fit(Xtr_bal, ytr_bal, verbose=False)\n",
        "    pred_xgb = xgb.predict(Xte)\n",
        "    metrics_log.append(evaluate_and_report(y_test, pred_xgb, \"XGBoost (TF-IDF)\", id2label))\n",
        "except Exception as e:\n",
        "    print(\"XGBoost skipped due to:\", e)\n",
        "\n",
        "# ==============================#\n",
        "#   Deep Learning: BiLSTM      #\n",
        "# ==============================#\n",
        "\n",
        "# Tokenization for Keras (keep emojis)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_VOCAB = 60000\n",
        "MAX_LEN   = 96\n",
        "\n",
        "tok = Tokenizer(num_words=MAX_VOCAB, oov_token=\"<OOV>\", filters='')  # keep punctuation & emojis\n",
        "tok.fit_on_texts(train_df['clean'].tolist())\n",
        "\n",
        "Xtr_seq = tok.texts_to_sequences(train_df['clean'].tolist())\n",
        "Xte_seq = tok.texts_to_sequences(test_df['clean'].tolist())\n",
        "\n",
        "Xtr_pad = pad_sequences(Xtr_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "Xte_pad = pad_sequences(Xte_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "ytr = tf.keras.utils.to_categorical(train_df['y'], num_classes=num_labels)\n",
        "yte = tf.keras.utils.to_categorical(test_df['y'], num_classes=num_labels)\n",
        "\n",
        "# Class weights for imbalance\n",
        "class_counts = Counter(train_df['y'].tolist())\n",
        "total = sum(class_counts.values())\n",
        "class_weight = {cls: total/(num_labels*cnt) for cls, cnt in class_counts.items()}\n",
        "print(\"Class weights (BiLSTM):\", class_weight)\n",
        "\n",
        "def build_bilstm_model(vocab_size, num_labels, max_len=MAX_LEN, emb_dim=128, lstm_units=128, rate=0.3):\n",
        "    inputs = keras.Input(shape=(max_len,), dtype='int32')\n",
        "    x = layers.Embedding(vocab_size, emb_dim, input_length=max_len, mask_zero=False)(inputs)\n",
        "    x = layers.SpatialDropout1D(rate)(x)\n",
        "    x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(rate)(x)\n",
        "    outputs = layers.Dense(num_labels, activation='softmax')(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=2e-3),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "bilstm = build_bilstm_model(vocab_size=min(MAX_VOCAB, len(tok.word_index)+1), num_labels=num_labels)\n",
        "bilstm.summary()\n",
        "\n",
        "cb = [\n",
        "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
        "]\n",
        "\n",
        "hist = bilstm.fit(\n",
        "    Xtr_pad, ytr,\n",
        "    validation_split=0.15,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    class_weight=class_weight,\n",
        "    verbose=1,\n",
        ")\n",
        "pred_bilstm = bilstm.predict(Xte_pad, batch_size=256)\n",
        "pred_bilstm_lbl = pred_bilstm.argmax(axis=1)\n",
        "metrics_log.append(evaluate_and_report(test_df['y'].values, pred_bilstm_lbl, \"BiLSTM (Keras)\", id2label))\n",
        "\n",
        "# ==============================#\n",
        "#  Deep Learning: DistilBERT   #\n",
        "# ==============================#\n",
        "\n",
        "MODEL_NAME = \"distilbert-base-uncased\"  # English baseline; works reasonably with Roman Urdu + emojis\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Build HuggingFace Dataset\n",
        "hf_train = Dataset.from_pandas(train_df.rename(columns={\"clean\":\"text\", \"y\":\"label\"}))\n",
        "hf_test  = Dataset.from_pandas(test_df.rename(columns={\"clean\":\"text\", \"y\":\"label\"}))\n",
        "\n",
        "def tok_func(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=False,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "hf_train = hf_train.map(tok_func, batched=True)\n",
        "hf_test  = hf_test.map(tok_func, batched=True)\n",
        "\n",
        "# Set format for PyTorch\n",
        "hf_train = hf_train.remove_columns([c for c in hf_train.column_names if c not in ['input_ids','attention_mask','label']])\n",
        "hf_test  = hf_test.remove_columns([c for c in hf_test.column_names if c not in ['input_ids','attention_mask','label']])\n",
        "hf_train.set_format(type='torch')\n",
        "hf_test.set_format(type='torch')\n",
        "\n",
        "id2label_hf = {i: id2label[i] for i in range(num_labels)}\n",
        "label2id_hf = {v: k for k, v in id2label_hf.items()}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME, num_labels=num_labels, id2label=id2label_hf, label2id=label2id_hf\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1m = f1_score(labels, preds, average='macro')\n",
        "    return {\"accuracy\": acc, \"macro_f1\": f1m}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/bert_out\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"macro_f1\",\n",
        "    greater_is_better=True,\n",
        "    report_to=\"none\"  # disable wandb by default\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=hf_train,\n",
        "    eval_dataset=hf_test,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "eval_res = trainer.evaluate()\n",
        "print(\"DistilBERT Eval:\", eval_res)\n",
        "\n",
        "# Predictions & full report\n",
        "bert_preds = np.argmax(trainer.predict(hf_test).predictions, axis=1)\n",
        "metrics_log.append(evaluate_and_report(test_df['y'].values, bert_preds, \"DistilBERT\", id2label))\n",
        "\n",
        "# ==============================#\n",
        "#       Save All Outputs        #\n",
        "# ==============================#\n",
        "\n",
        "# Metrics table\n",
        "metrics_df = pd.DataFrame(metrics_log).sort_values(by=\"macro_f1\", ascending=False)\n",
        "metrics_csv_path = \"/content/model_metrics.csv\"\n",
        "metrics_df.to_csv(metrics_csv_path, index=False)\n",
        "print(\"\\nSaved metrics to:\", metrics_csv_path)\n",
        "display(metrics_df)\n",
        "\n",
        "# Per-model predictions\n",
        "out_dir = \"/content/preds\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "def save_predictions(name, y_pred):\n",
        "    out = test_df.copy()\n",
        "    out['pred_id'] = y_pred\n",
        "    out['pred_label'] = out['pred_id'].map(id2label)\n",
        "    out['true_label'] = out['y'].map(id2label)\n",
        "    p = os.path.join(out_dir, f\"{name.replace(' ','_')}_preds.csv\")\n",
        "    out[[ 'clean','true_label','pred_label' ]].to_csv(p, index=False)\n",
        "    print(\"Saved:\", p)\n",
        "\n",
        "save_predictions(\"MultinomialNB\", pred_nb)\n",
        "save_predictions(\"LinearSVC\", pred_svm)\n",
        "save_predictions(\"LogisticRegression\", pred_lr)\n",
        "save_predictions(\"RandomForest\", pred_rf)\n",
        "try:\n",
        "    save_predictions(\"XGBoost\", pred_xgb)\n",
        "except:\n",
        "    pass\n",
        "save_predictions(\"BiLSTM\", pred_bilstm_lbl)\n",
        "save_predictions(\"DistilBERT\", bert_preds)\n",
        "\n",
        "print(\"\\nDone. Check /content/model_metrics.csv and /content/preds/*.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUqxOcSahwuVaJFfZovKly",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}