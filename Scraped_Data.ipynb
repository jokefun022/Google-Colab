{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMndZTQTDLOsd5ZWKxZJiqS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jokefun022/Google-Colab/blob/main/Scraped_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open(\"tweets.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Tweet ID\", \"Text\"])\n",
        "    # Iterate directly over the tweets list\n",
        "    for tweet_data in tweets:\n",
        "        # Based on your scraping code, tweets is a list of lists, where each inner\n",
        "        # list is [tweet.date, tweet.content]. You want to write the id and text.\n",
        "        # However, your previous scraping code was appending [tweet.date, tweet.content].\n",
        "        # The traceback suggests you might have intended to use an object with .id and .text.\n",
        "        # Let's assume the intended structure is that `tweets` is a list of objects/dictionaries\n",
        "        # that have 'id' and 'text' attributes/keys. If the structure is [date, content],\n",
        "        # you'll need to adjust how you access the data below.\n",
        "        # Assuming tweet_data is an object with .id and .text attributes (like the original snscrape Tweet object):\n",
        "        # writer.writerow([tweet_data.id, tweet_data.text])\n",
        "\n",
        "        # Based on your provided scraping loop:\n",
        "        # tweets.append([tweet.date, tweet.content])\n",
        "        # This means `tweets` is a list of lists, where each inner list is [date, content].\n",
        "        # You cannot access `.id` and `.text` on these inner lists.\n",
        "        # It appears there's a mismatch between how you populate `tweets` and how you try to use it here.\n",
        "\n",
        "        # To fix this based on how you *are* populating `tweets` (with [date, content]),\n",
        "        # you need to decide which fields you want to write to the CSV.\n",
        "        # If you want the content, it's the second element (index 1) of the inner list.\n",
        "        # The original header was \"Tweet ID\", \"Text\". This suggests you want the ID and Text.\n",
        "        # The way `tweets` is populated currently ([date, content]) does not include the ID.\n",
        "\n",
        "        # Let's revise the scraping part to include the ID and text, consistent with the CSV writing part.\n",
        "        # Original snscrape objects have `id` and `rawContent` (or `content`).\n",
        "        # Modify the scraping loop earlier in your notebook to:\n",
        "        # for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
        "        #     if i > 100:\n",
        "        #         break\n",
        "        #     tweets.append([tweet.id, tweet.rawContent]) # Or tweet.content depending on the exact version\n",
        "        #     time.sleep(2)\n",
        "\n",
        "        # With the scraping code modified as above, the tweet_data will be a list [tweet.id, tweet.rawContent].\n",
        "        # Then, you can access the ID and Text by index:\n",
        "        writer.writerow([tweet_data[0], tweet_data[1]])\n",
        "\n",
        "# If you cannot easily change the scraping code that produces the `tweets` list\n",
        "# and it *is* a list of objects that have `id` and `text` attributes,\n",
        "# then the fix is simply removing `.data`:\n",
        "# for tweet in tweets:\n",
        "#     writer.writerow([tweet.id, tweet.text])\n",
        "\n",
        "# Given the traceback and the global variable state, the most likely scenario is that `tweets` was\n",
        "# populated incorrectly for this writing loop, or the loop is written assuming a different structure for `tweets`.\n",
        "# The fix below assumes you can modify the scraping loop to store [tweet.id, tweet.rawContent].\n"
      ],
      "metadata": {
        "id": "16oQYhg-4-h2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install snscrape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUpaWSMx5ONE",
        "outputId": "94eaf746-3297-4e2b-d65e-90180d89d5be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: snscrape in /usr/local/lib/python3.11/dist-packages (0.7.0.20230622)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from snscrape) (2.32.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from snscrape) (5.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from snscrape) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from snscrape) (3.18.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->snscrape) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->snscrape) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (2025.4.26)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!snscrape --jsonl --max-results 1000 --progress twitter-search \"tum ❤️ since:2024-01-01 until:2025-01-01\" > tweets.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKCLN9i05Sxz",
        "outputId": "b23d4cda-2063-44c0-8a93-55a977198cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-30 11:13:45.132  ERROR  snscrape.base  Error retrieving https://twitter.com/search?f=live&lang=en&q=tum+%E2%9D%A4%EF%B8%8F+since%3A2024-01-01+until%3A2025-01-01&src=spelling_expansion_revert_click: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=tum+%E2%9D%A4%EF%B8%8F+since%3A2024-01-01+until%3A2025-01-01&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\"))\n",
            "2025-05-30 11:13:45.133  CRITICAL  snscrape.base  4 requests to https://twitter.com/search?f=live&lang=en&q=tum+%E2%9D%A4%EF%B8%8F+since%3A2024-01-01+until%3A2025-01-01&src=spelling_expansion_revert_click failed, giving up.\n",
            "2025-05-30 11:13:45.133  CRITICAL  snscrape.base  Errors: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=tum+%E2%9D%A4%EF%B8%8F+since%3A2024-01-01+until%3A2025-01-01&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=tum+%E2%9D%A4%EF%B8%8F+since%3A2024-01-01+until%3A2025-01-01&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=tum+%E2%9D%A4%EF%B8%8F+since%3A2024-01-01+until%3A2025-01-01&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=tum+%E2%9D%A4%EF%B8%8F+since%3A2024-01-01+until%3A2025-01-01&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\"))\n",
            "2025-05-30 11:13:45.155  CRITICAL  snscrape._cli  Dumped stack and locals to /tmp/snscrape_locals_fy066hhj\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/snscrape\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/snscrape/_cli.py\", line 323, in main\n",
            "    for i, item in enumerate(scraper.get_items(), start = 1):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/snscrape/modules/twitter.py\", line 1763, in get_items\n",
            "    for obj in self._iter_api_data('https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline', _TwitterAPIType.GRAPHQL, params, paginationParams, cursor = self._cursor, instructionsPath = ['data', 'search_by_raw_query', 'search_timeline', 'timeline', 'instructions']):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/snscrape/modules/twitter.py\", line 915, in _iter_api_data\n",
            "    obj = self._get_api_data(endpoint, apiType, reqParams, instructionsPath = instructionsPath)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/snscrape/modules/twitter.py\", line 883, in _get_api_data\n",
            "    self._ensure_guest_token()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/snscrape/modules/twitter.py\", line 825, in _ensure_guest_token\n",
            "    r = self._get(self._baseUrl if url is None else url, responseOkCallback = self._check_guest_token_response)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/snscrape/base.py\", line 275, in _get\n",
            "    return self._request('GET', *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/snscrape/base.py\", line 271, in _request\n",
            "    raise ScraperException(msg)\n",
            "snscrape.base.ScraperException: 4 requests to https://twitter.com/search?f=live&lang=en&q=tum+%E2%9D%A4%EF%B8%8F+since%3A2024-01-01+until%3A2025-01-01&src=spelling_expansion_revert_click failed, giving up.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cW98RM8l0qb_"
      },
      "outputs": [],
      "source": [
        "!pip install snscrape\n",
        "\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "\n",
        "query = \"elonmusk since:2024-01-01 until:2024-12-31\"\n",
        "tweets = []\n",
        "for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
        "    tweets.append([tweet.date, tweet.content])\n",
        "!pip install snscrape\n",
        "\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "\n",
        "query = \"elonmusk since:2024-01-01 until:2024-12-31\"\n",
        "tweets = []\n",
        "for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
        "    tweets.append([tweet.date, tweet.content])\n",
        "\n",
        "print(tweets[:5])\n",
        "!snscrape --max-results 100 twitter-search \"from:elonmusk since:2024-01-01 until:2024-12-31\"\n",
        "# Or in Python:\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "\n",
        "query = \"elonmusk since:2024-01-01 until:2024-12-31\"\n",
        "tweets = []\n",
        "for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
        "    tweets.append([tweet.date, tweet.content])\n",
        "\n",
        "print(tweets[:5])\n",
        "\n",
        "query = \"elonmusk since:2024-01-01 until:2024-12-31\"\n",
        "tweets = []\n",
        "for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
        "    tweets.append([tweet.date, tweet.content])\n",
        "\n",
        "print(tweets[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "!pip install --upgrade snscrape  # Upgrade snscrape\n",
        "\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import time # Import the time module\n",
        "\n",
        "query = \"elonmusk since:2024-01-01 until:2024-12-31\"\n",
        "tweets = []\n",
        "# Limit the number of tweets to try and reduce the chance of failure\n",
        "for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
        "    # Stop after a reasonable number of tweets to avoid being blocked\n",
        "    if i > 100: # You can adjust this number\n",
        "        break\n",
        "    tweets.append([tweet.date, tweet.content])\n",
        "    # Add a delay between requests\n",
        "    time.sleep(2) # Increased delay to 2 seconds, adjust as needed\n",
        "\n",
        "print(tweets[:5])\n",
        "\n",
        "# The following parts of your original code were redundant or unlikely to work\n",
        "# reliably due to the scraping issues, so they are commented out or removed.\n",
        "\n",
        "# !pip install snscrape # Redundant installation\n",
        "\n",
        "# import snscrape.modules.twitter as sntwitter # Redundant import\n",
        "\n",
        "# query = \"elonmusk since:2024-01-01 until:2024-12-31\"\n",
        "# tweets = []\n",
        "# for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
        "#     tweets.append([tweet.date, tweet.content])\n",
        "\n",
        "# print(tweets[:5])\n",
        "# !snscrape --max-results 100 twitter-search \"from:elonmusk since:2024-01-01 until:2024-12-31\" # This might still fail\n",
        "# # Or in Python:\n",
        "# import snscrape.modules.twitter as sntwitter # Redundant import\n",
        "\n",
        "# query = \"elonmusk since:2024-01-01 until:2024-12-31\"\n",
        "# tweets = []\n",
        "# for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
        "#     tweets.append([tweet.date, tweet.content])\n",
        "\n",
        "# print(tweets[:5])\n",
        "\n",
        "# query = \"elonmusk since:2024-01-01 until:2024-12-31\"\n",
        "# tweets = []\n",
        "# for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
        "#     tweets.append([tweet.date, tweet.content])\n",
        "\n",
        "# print(tweets[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "LDEO6f2g1K68",
        "outputId": "95b48e98-4fd8-4bd7-8972-be9a80c41bce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: snscrape in /usr/local/lib/python3.11/dist-packages (0.7.0.20230622)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from snscrape) (2.32.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from snscrape) (5.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from snscrape) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from snscrape) (3.18.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->snscrape) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->snscrape) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (2025.4.26)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->snscrape) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:snscrape.base:Error retrieving https://twitter.com/search?f=live&lang=en&q=elonmusk+since%3A2024-01-01+until%3A2024-12-31&src=spelling_expansion_revert_click: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=elonmusk+since%3A2024-01-01+until%3A2024-12-31&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\"))\n",
            "CRITICAL:snscrape.base:4 requests to https://twitter.com/search?f=live&lang=en&q=elonmusk+since%3A2024-01-01+until%3A2024-12-31&src=spelling_expansion_revert_click failed, giving up.\n",
            "CRITICAL:snscrape.base:Errors: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=elonmusk+since%3A2024-01-01+until%3A2024-12-31&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=elonmusk+since%3A2024-01-01+until%3A2024-12-31&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=elonmusk+since%3A2024-01-01+until%3A2024-12-31&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=elonmusk+since%3A2024-01-01+until%3A2024-12-31&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\"))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ScraperException",
          "evalue": "4 requests to https://twitter.com/search?f=live&lang=en&q=elonmusk+since%3A2024-01-01+until%3A2024-12-31&src=spelling_expansion_revert_click failed, giving up.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mScraperException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d4d73a632d49>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Limit the number of tweets to try and reduce the chance of failure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msntwitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTwitterSearchScraper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Stop after a reasonable number of tweets to avoid being blocked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# You can adjust this number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snscrape/modules/twitter.py\u001b[0m in \u001b[0;36mget_items\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1761\u001b[0m                 \u001b[0mpaginationParams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'variables'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpaginationVariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'features'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1763\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_api_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_TwitterAPIType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRAPHQL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaginationParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstructionsPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'search_by_raw_query'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'search_timeline'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'timeline'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'instructions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1764\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graphql_timeline_instructions_to_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'search_by_raw_query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'search_timeline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timeline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'instructions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snscrape/modules/twitter.py\u001b[0m in \u001b[0;36m_iter_api_data\u001b[0;34m(self, endpoint, apiType, params, paginationParams, cursor, direction, instructionsPath)\u001b[0m\n\u001b[1;32m    913\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m                         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Retrieving scroll page {cursor}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m                         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_api_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapiType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreqParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstructionsPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstructionsPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snscrape/modules/twitter.py\u001b[0m in \u001b[0;36m_get_api_data\u001b[0;34m(self, endpoint, apiType, params, instructionsPath)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_get_api_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapiType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstructionsPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_guest_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mapiType\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_TwitterAPIType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRAPHQL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquote_via\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquote\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snscrape/modules/twitter.py\u001b[0m in \u001b[0;36m_ensure_guest_token\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guestTokenManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m                         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Retrieving guest token'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_baseUrl\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponseOkCallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_guest_token_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'document\\.cookie = decodeURIComponent\\(\"gt=(\\d+); Max-Age=10800; Domain=\\.twitter\\.com; Path=/; Secure\"\\);'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                                 \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found guest token in HTML'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snscrape/base.py\u001b[0m in \u001b[0;36m_get\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snscrape/base.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[1;32m    269\u001b[0m                         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Errors: {\", \".join(errors)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mScraperException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reached unreachable code'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mScraperException\u001b[0m: 4 requests to https://twitter.com/search?f=live&lang=en&q=elonmusk+since%3A2024-01-01+until%3A2024-12-31&src=spelling_expansion_revert_click failed, giving up."
          ]
        }
      ]
    }
  ]
}